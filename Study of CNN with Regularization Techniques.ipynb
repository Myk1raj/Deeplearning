{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22BAI1118 \n",
    "Mayank Raj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the Fashion MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the images to the range [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape for CNN (28x28 grayscale images to 28x28x1)\n",
    "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))\n",
    "\n",
    "# Create a validation set from the training set\n",
    "val_size = 10000\n",
    "x_val, x_train = x_train[:val_size], x_train[val_size:]\n",
    "y_val, y_train = y_train[:val_size], y_train[val_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create CNN model with different configurations\n",
    "def create_model(regularization=None, dropout_rate_conv1=0.3, dropout_rate_conv2=0.0, dropout_rate_dense=0.5):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), kernel_regularizer=regularization),\n",
    "        Dropout(dropout_rate_conv1),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularization),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularization),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu', kernel_regularizer=regularization),\n",
    "        Dropout(dropout_rate_dense),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate model\n",
    "def train_and_evaluate(model, x_train, y_train, x_val, y_val, epochs=20, use_datagen=False):\n",
    "    if use_datagen:\n",
    "        history = model.fit(\n",
    "            datagen.flow(x_train, y_train, batch_size=64),\n",
    "            steps_per_epoch=len(x_train) // 64,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "    else:\n",
    "        history = model.fit(\n",
    "            x_train, y_train,\n",
    "            batch_size=64,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Load the Fashion MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Define class names for the dataset\n",
    "class_names = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]\n",
    "\n",
    "# Plot a few sample images from the dataset\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[y_train[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model without regularization or dropout\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MYK\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.6269 - loss: 1.0241 - val_accuracy: 0.8177 - val_loss: 0.5090\n",
      "Epoch 2/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8134 - loss: 0.5187 - val_accuracy: 0.8449 - val_loss: 0.4279\n",
      "Epoch 3/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8368 - loss: 0.4429 - val_accuracy: 0.8584 - val_loss: 0.3929\n",
      "Epoch 4/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8589 - loss: 0.3893 - val_accuracy: 0.8721 - val_loss: 0.3550\n",
      "Epoch 5/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8689 - loss: 0.3595 - val_accuracy: 0.8721 - val_loss: 0.3475\n",
      "Epoch 6/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8769 - loss: 0.3366 - val_accuracy: 0.8832 - val_loss: 0.3239\n",
      "Epoch 7/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8816 - loss: 0.3184 - val_accuracy: 0.8876 - val_loss: 0.3139\n",
      "Epoch 8/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8894 - loss: 0.2997 - val_accuracy: 0.8933 - val_loss: 0.2957\n",
      "Epoch 9/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8962 - loss: 0.2829 - val_accuracy: 0.8939 - val_loss: 0.2901\n",
      "Epoch 10/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8996 - loss: 0.2727 - val_accuracy: 0.8964 - val_loss: 0.2881\n",
      "Epoch 11/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9062 - loss: 0.2570 - val_accuracy: 0.8957 - val_loss: 0.2814\n",
      "Epoch 12/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9081 - loss: 0.2449 - val_accuracy: 0.8927 - val_loss: 0.2872\n",
      "Epoch 13/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9137 - loss: 0.2312 - val_accuracy: 0.8972 - val_loss: 0.2778\n",
      "Epoch 14/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9174 - loss: 0.2248 - val_accuracy: 0.8978 - val_loss: 0.2826\n",
      "Epoch 15/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9172 - loss: 0.2199 - val_accuracy: 0.9007 - val_loss: 0.2723\n",
      "Epoch 16/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9212 - loss: 0.2124 - val_accuracy: 0.8987 - val_loss: 0.2792\n",
      "Epoch 17/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9232 - loss: 0.2002 - val_accuracy: 0.8993 - val_loss: 0.2788\n",
      "Epoch 18/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9279 - loss: 0.1940 - val_accuracy: 0.8897 - val_loss: 0.3051\n",
      "Epoch 19/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9278 - loss: 0.1932 - val_accuracy: 0.8910 - val_loss: 0.3160\n",
      "Epoch 20/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9270 - loss: 0.1920 - val_accuracy: 0.8910 - val_loss: 0.3209\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model without any regularization or dropout\n",
    "print(\"Training model without regularization or dropout\")\n",
    "model_no_reg = create_model()\n",
    "history_no_reg = train_and_evaluate(model_no_reg, x_train, y_train, x_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference \n",
    "The model trained without regularization or dropout achieved a final training accuracy of 92.70% and a validation accuracy of 89.10%. Both training and validation losses decreased over the epochs, indicating effective learning. However, the increasing validation loss towards the end suggests potential overfitting. This baseline performance highlights the need for regularization techniques to improve generalization and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with L1 regularization\n",
      "Epoch 1/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.1039 - loss: 8.9902 - val_accuracy: 0.1000 - val_loss: 2.4370\n",
      "Epoch 2/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.0984 - loss: 2.4369 - val_accuracy: 0.0990 - val_loss: 2.4361\n",
      "Epoch 3/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.0994 - loss: 2.4367 - val_accuracy: 0.0942 - val_loss: 2.4372\n",
      "Epoch 4/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.0980 - loss: 2.4367 - val_accuracy: 0.0989 - val_loss: 2.4364\n",
      "Epoch 5/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.0995 - loss: 2.4366 - val_accuracy: 0.1019 - val_loss: 2.4370\n",
      "Training model with L2 regularization\n",
      "Epoch 1/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.5899 - loss: 1.9373 - val_accuracy: 0.7469 - val_loss: 0.9961\n",
      "Epoch 2/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.7590 - loss: 0.9366 - val_accuracy: 0.7824 - val_loss: 0.8786\n",
      "Epoch 3/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.7715 - loss: 0.8592 - val_accuracy: 0.7732 - val_loss: 0.8658\n",
      "Epoch 4/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.7738 - loss: 0.8321 - val_accuracy: 0.7894 - val_loss: 0.8020\n",
      "Epoch 5/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.7838 - loss: 0.8020 - val_accuracy: 0.7901 - val_loss: 0.7889\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate models with L1 and L2 regularization\n",
    "results = []\n",
    "\n",
    "for reg_type, reg in zip([\"L1\", \"L2\"], [l1(0.01), l2(0.01)]):\n",
    "    print(f\"Training model with {reg_type} regularization\")\n",
    "    model_reg = create_model(regularization=reg)\n",
    "    history_reg = train_and_evaluate(model_reg, x_train, y_train, x_val, y_val)\n",
    "    results.append([\n",
    "        f\"{reg_type} Regularization\",\n",
    "        history_reg.history['val_accuracy'][-1],\n",
    "        history_reg.history['val_loss'][-1]\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference \n",
    "The L1 regularization model showed poor performance, with both training and validation accuracies stagnating around 10%, indicating that the model did not learn effectively. In contrast, the L2 regularization model demonstrated better performance, achieving a final training accuracy of 78.38% and a validation accuracy of 79.01%. The L2 regularization effectively improved the model’s learning and generalization compared to L1 regularization. This suggests that L2 regularization was more effective in managing overfitting and optimizing the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with Dropout 0.2-0.0-0.5\n",
      "Epoch 1/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.6276 - loss: 1.0296 - val_accuracy: 0.8230 - val_loss: 0.4950\n",
      "Epoch 2/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8194 - loss: 0.5083 - val_accuracy: 0.8448 - val_loss: 0.4121\n",
      "Epoch 3/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8450 - loss: 0.4305 - val_accuracy: 0.8548 - val_loss: 0.3881\n",
      "Epoch 4/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.8619 - loss: 0.3861 - val_accuracy: 0.8761 - val_loss: 0.3432\n",
      "Epoch 5/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.8709 - loss: 0.3561 - val_accuracy: 0.8814 - val_loss: 0.3425\n",
      "Training model with Dropout 0.3-0.0-0.5\n",
      "Epoch 1/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.6340 - loss: 1.0167 - val_accuracy: 0.8130 - val_loss: 0.5181\n",
      "Epoch 2/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.8129 - loss: 0.5187 - val_accuracy: 0.8538 - val_loss: 0.4257\n",
      "Epoch 3/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8417 - loss: 0.4344 - val_accuracy: 0.8635 - val_loss: 0.3715\n",
      "Epoch 4/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.8622 - loss: 0.3836 - val_accuracy: 0.8747 - val_loss: 0.3465\n",
      "Epoch 5/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8663 - loss: 0.3606 - val_accuracy: 0.8751 - val_loss: 0.3311\n",
      "Training model with No Dropout\n",
      "Epoch 1/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.6846 - loss: 0.8727 - val_accuracy: 0.8370 - val_loss: 0.4511\n",
      "Epoch 2/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8374 - loss: 0.4461 - val_accuracy: 0.8576 - val_loss: 0.3831\n",
      "Epoch 3/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8631 - loss: 0.3668 - val_accuracy: 0.8720 - val_loss: 0.3506\n",
      "Epoch 4/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8797 - loss: 0.3284 - val_accuracy: 0.8794 - val_loss: 0.3243\n",
      "Epoch 5/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8878 - loss: 0.3013 - val_accuracy: 0.8830 - val_loss: 0.3184\n"
     ]
    }
   ],
   "source": [
    "# Define dropout configurations\n",
    "dropout_configs = [\n",
    "    {\"dropout_rate_conv1\": 0.2, \"dropout_rate_conv2\": 0.0, \"dropout_rate_dense\": 0.5, \"name\": \"Dropout 0.2-0.0-0.5\"},\n",
    "    {\"dropout_rate_conv1\": 0.3, \"dropout_rate_conv2\": 0.0, \"dropout_rate_dense\": 0.5, \"name\": \"Dropout 0.3-0.0-0.5\"},\n",
    "    {\"dropout_rate_conv1\": 0.0, \"dropout_rate_conv2\": 0.0, \"dropout_rate_dense\": 0.0, \"name\": \"No Dropout\"}\n",
    "]\n",
    "\n",
    "# Train and evaluate models with different dropout configurations\n",
    "for config in dropout_configs:\n",
    "    print(f\"Training model with {config['name']}\")\n",
    "    model_dropout = create_model(\n",
    "        dropout_rate_conv1=config[\"dropout_rate_conv1\"],\n",
    "        dropout_rate_conv2=config[\"dropout_rate_conv2\"],\n",
    "        dropout_rate_dense=config[\"dropout_rate_dense\"]\n",
    "    )\n",
    "    history_dropout = train_and_evaluate(model_dropout, x_train, y_train, x_val, y_val)\n",
    "    results.append([\n",
    "        config[\"name\"],\n",
    "        history_dropout.history['val_accuracy'][-1],\n",
    "        history_dropout.history['val_loss'][-1]\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference \n",
    "The models trained with dropout configurations exhibited varying performances:\n",
    "\n",
    "Dropout 0.2-0.0-0.5: This configuration achieved a high validation accuracy of 88.14% and a validation loss of 0.3425. The model effectively utilized dropout to improve performance, especially in reducing overfitting.\n",
    "\n",
    "Dropout 0.3-0.0-0.5: With a dropout rate of 0.3 for the first convolutional layer, this model had a slightly lower validation accuracy of 87.51% and a validation loss of 0.3311. Although the performance was good, it was not as high as the 0.2-0.0-0.5 configuration.\n",
    "\n",
    "No Dropout: The model without dropout achieved the highest validation accuracy of 88.30% and a validation loss of 0.3184. This indicates that for this particular dataset and model architecture, not applying dropout was the most effective strategy, possibly because the model was not overfitting significantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model without regularization or dropout with data augmentation\n",
      "Epoch 1/20\n",
      "\u001b[1m 11/781\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.0978 - loss: 2.2943   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MYK\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.4353 - loss: 1.5190 - val_accuracy: 0.7353 - val_loss: 0.7111\n",
      "Epoch 2/20\n",
      "\u001b[1m  1/781\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.5938 - loss: 1.0786"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 753us/step - accuracy: 0.5938 - loss: 1.0786 - val_accuracy: 0.7305 - val_loss: 0.7173\n",
      "Epoch 3/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.6594 - loss: 0.9255 - val_accuracy: 0.7582 - val_loss: 0.6437\n",
      "Epoch 4/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 859us/step - accuracy: 0.7500 - loss: 0.6599 - val_accuracy: 0.7583 - val_loss: 0.6419\n",
      "Epoch 5/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.6962 - loss: 0.8240 - val_accuracy: 0.7742 - val_loss: 0.5928\n",
      "Epoch 6/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 827us/step - accuracy: 0.7656 - loss: 0.5869 - val_accuracy: 0.7719 - val_loss: 0.5981\n",
      "Epoch 7/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.7154 - loss: 0.7738 - val_accuracy: 0.7789 - val_loss: 0.5625\n",
      "Epoch 8/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 757us/step - accuracy: 0.7031 - loss: 0.7551 - val_accuracy: 0.7748 - val_loss: 0.5672\n",
      "Epoch 9/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.7267 - loss: 0.7437 - val_accuracy: 0.7890 - val_loss: 0.5580\n",
      "Epoch 10/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 829us/step - accuracy: 0.7344 - loss: 0.6349 - val_accuracy: 0.7958 - val_loss: 0.5494\n",
      "Epoch 11/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.7396 - loss: 0.7048 - val_accuracy: 0.7734 - val_loss: 0.5748\n",
      "Epoch 12/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 831us/step - accuracy: 0.7656 - loss: 0.5472 - val_accuracy: 0.7855 - val_loss: 0.5543\n",
      "Epoch 13/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.7499 - loss: 0.6836 - val_accuracy: 0.8052 - val_loss: 0.5112\n",
      "Epoch 14/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 813us/step - accuracy: 0.7500 - loss: 0.6878 - val_accuracy: 0.8072 - val_loss: 0.5078\n",
      "Epoch 15/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.7606 - loss: 0.6586 - val_accuracy: 0.8162 - val_loss: 0.4931\n",
      "Epoch 16/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 846us/step - accuracy: 0.7656 - loss: 0.5829 - val_accuracy: 0.8177 - val_loss: 0.4893\n",
      "Epoch 17/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.7686 - loss: 0.6369 - val_accuracy: 0.8185 - val_loss: 0.4823\n",
      "Epoch 18/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 823us/step - accuracy: 0.7969 - loss: 0.6648 - val_accuracy: 0.8225 - val_loss: 0.4741\n",
      "Epoch 19/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.7720 - loss: 0.6267 - val_accuracy: 0.8166 - val_loss: 0.5100\n",
      "Epoch 20/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 814us/step - accuracy: 0.7969 - loss: 0.5154 - val_accuracy: 0.8164 - val_loss: 0.5106\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model without any regularization or dropout but with data augmentation\n",
    "print(\"Training model without regularization or dropout with data augmentation\")\n",
    "model_no_reg_aug = create_model()\n",
    "history_no_reg_aug = train_and_evaluate(model_no_reg_aug, x_train, y_train, x_val, y_val, use_datagen=True)\n",
    "results.append([\n",
    "    \"No Regularization or Dropout with Augmentation\",\n",
    "    history_no_reg_aug.history['val_accuracy'][-1],\n",
    "    history_no_reg_aug.history['val_loss'][-1]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference \n",
    "The model trained without regularization or dropout but with data augmentation demonstrated notable improvements in performance. Over the course of 20 epochs, the model's validation accuracy steadily increased, reaching a peak of 82.25% by epoch 18. Concurrently, the validation loss showed a consistent decline, achieving a final value of 0.4741 at the end of the training.\n",
    "\n",
    "During the initial epochs, validation accuracy rose from 73.05% to 77.42%, and validation loss improved from 0.7173 to 0.5928. As training progressed, the model's performance continued to enhance, with accuracy reaching up to 79.00% and validation loss dropping to 0.5494 by epoch 10. The trend of improvement persisted, with the model achieving a validation accuracy of 81.62% and a loss of 0.4931 by epoch 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model without regularization or dropout with early stopping\n",
      "Epoch 1/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.6203 - loss: 1.0342 - val_accuracy: 0.8318 - val_loss: 0.4978\n",
      "Epoch 2/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8193 - loss: 0.4968 - val_accuracy: 0.8538 - val_loss: 0.4018\n",
      "Epoch 3/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8506 - loss: 0.4162 - val_accuracy: 0.8628 - val_loss: 0.3812\n",
      "Epoch 4/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8623 - loss: 0.3808 - val_accuracy: 0.8729 - val_loss: 0.3503\n",
      "Epoch 5/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8729 - loss: 0.3543 - val_accuracy: 0.8859 - val_loss: 0.3247\n",
      "Epoch 6/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8792 - loss: 0.3303 - val_accuracy: 0.8766 - val_loss: 0.3346\n",
      "Epoch 7/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8865 - loss: 0.3102 - val_accuracy: 0.8842 - val_loss: 0.3212\n",
      "Epoch 8/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8914 - loss: 0.2929 - val_accuracy: 0.8790 - val_loss: 0.3209\n",
      "Epoch 9/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8960 - loss: 0.2778 - val_accuracy: 0.8905 - val_loss: 0.2968\n",
      "Epoch 10/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9036 - loss: 0.2610 - val_accuracy: 0.8973 - val_loss: 0.2813\n",
      "Epoch 11/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9073 - loss: 0.2518 - val_accuracy: 0.8977 - val_loss: 0.2800\n",
      "Epoch 12/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9137 - loss: 0.2312 - val_accuracy: 0.9001 - val_loss: 0.2737\n",
      "Epoch 13/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9159 - loss: 0.2267 - val_accuracy: 0.8959 - val_loss: 0.2783\n",
      "Epoch 14/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9166 - loss: 0.2205 - val_accuracy: 0.8976 - val_loss: 0.2792\n",
      "Epoch 15/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9209 - loss: 0.2110 - val_accuracy: 0.8919 - val_loss: 0.2912\n",
      "Epoch 16/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9215 - loss: 0.2079 - val_accuracy: 0.8971 - val_loss: 0.2835\n",
      "Epoch 17/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9238 - loss: 0.2044 - val_accuracy: 0.8947 - val_loss: 0.2949\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model without any regularization or dropout but with early stopping\n",
    "print(\"Training model without regularization or dropout with early stopping\")\n",
    "model_no_reg_es = create_model()\n",
    "history_no_reg_es = train_and_evaluate(model_no_reg_es, x_train, y_train, x_val, y_val)\n",
    "results.append([\n",
    "    \"No Regularization or Dropout with Early Stopping\",\n",
    "    history_no_reg_es.history['val_accuracy'][-1],\n",
    "    history_no_reg_es.history['val_loss'][-1]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferecene \n",
    "During training, the model demonstrated significant improvement in both accuracy and loss metrics. In the first epoch, the model achieved a training accuracy of 62.03% and a loss of 1.0342, with validation accuracy at 83.18% and a validation loss of 0.4978. By the second epoch, the training accuracy had improved to 81.93% with a loss of 0.4968, while the validation accuracy increased to 85.38% and the validation loss decreased to 0.4018. The model continued to show progress throughout the subsequent epochs, reaching a training accuracy of 90.36% by the tenth epoch, with a validation accuracy peaking at 89.73% and a validation loss of 0.2813. Over the course of the training, the model’s performance stabilized, with training accuracy reaching up to 92.38% and validation accuracy peaking at 90.01%. The early stopping mechanism was activated before reaching the maximum 20 epochs, likely due to the lack of significant improvement in the validation metrics. This approach helped prevent overfitting by halting training when further performance gains were minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance:\n",
      "+--------------------------------------------------+---------------------+---------------------+\n",
      "|                    Model Name                    | Validation Accuracy |   Validation Loss   |\n",
      "+--------------------------------------------------+---------------------+---------------------+\n",
      "|                L1 Regularization                 | 0.10189999639987946 | 2.4369752407073975  |\n",
      "|                L2 Regularization                 | 0.7900999784469604  |  0.788922131061554  |\n",
      "|               Dropout 0.2-0.0-0.5                | 0.8813999891281128  | 0.34245818853378296 |\n",
      "|               Dropout 0.3-0.0-0.5                | 0.8751000165939331  | 0.3311263918876648  |\n",
      "|                    No Dropout                    | 0.8830000162124634  | 0.31839755177497864 |\n",
      "|  No Regularization or Dropout with Augmentation  | 0.8163999915122986  | 0.5105697512626648  |\n",
      "| No Regularization or Dropout with Early Stopping | 0.8946999907493591  | 0.29494422674179077 |\n",
      "+--------------------------------------------------+---------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "# Print results as a table\n",
    "headers = [\"Model Name\", \"Validation Accuracy\", \"Validation Loss\"]\n",
    "print(\"\\nModel Performance:\")\n",
    "print(tabulate(results, headers=headers, tablefmt='pretty'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "1. L1 Regularization:\n",
    "\n",
    "Validation Accuracy: 0.1019\n",
    "\n",
    "Validation Loss: 2.4370\n",
    "\n",
    "Conclusion: L1 regularization led to a significant reduction in accuracy and an increase in loss. This suggests that L1 regularization might be too aggressive for this specific task or dataset, resulting in underfitting.\n",
    "______________________________________________________________\n",
    "\n",
    "2. L2 Regularization:\n",
    "\n",
    "Validation Accuracy: 0.7901\n",
    "\n",
    "Validation Loss: 0.7889\n",
    "\n",
    "Conclusion: L2 regularization improved performance compared to L1 regularization but did not achieve the highest accuracy. It provided a moderate regularization effect, balancing between underfitting and overfitting.\n",
    "\n",
    "______________________________________________________________\n",
    "\n",
    "3. Dropout (0.2-0.0-0.5):\n",
    "\n",
    "Validation Accuracy: 0.8814\n",
    "\n",
    "Validation Loss: 0.3425\n",
    "\n",
    "Conclusion: This dropout configuration demonstrated strong performance, indicating that dropout is effective in preventing overfitting while maintaining high accuracy.\n",
    "\n",
    "______________________________________________________________\n",
    "\n",
    "4. Dropout (0.3-0.0-0.5):\n",
    "\n",
    "Validation Accuracy: 0.8751\n",
    "\n",
    "Validation Loss: 0.3311\n",
    "\n",
    "Conclusion: A slightly different dropout configuration also performed well, but slightly less effectively than the previous dropout setting. This suggests that the specific dropout rates and configurations can have a noticeable impact on model performance.\n",
    "\n",
    "______________________________________________________________\n",
    "\n",
    "5. No Dropout:\n",
    "\n",
    "Validation Accuracy: 0.8830\n",
    "\n",
    "Validation Loss: 0.3184\n",
    "\n",
    "Conclusion: The model without dropout achieved the highest accuracy among the dropout configurations, indicating that for this dataset, dropout might have been too aggressive and hindered performance.\n",
    "\n",
    "______________________________________________________________\n",
    "\n",
    "6. No Regularization or Dropout with Augmentation:\n",
    "\n",
    "Validation Accuracy: 0.8164\n",
    "\n",
    "Validation Loss: 0.5106\n",
    "\n",
    "Conclusion: Data augmentation alone did not achieve as high accuracy as some of the regularization or dropout methods. This suggests that while augmentation can be beneficial, it might not always be sufficient on its own without other forms of regularization.\n",
    "\n",
    "______________________________________________________________\n",
    "\n",
    "7. No Regularization or Dropout with Early Stopping:\n",
    "\n",
    "Validation Accuracy: 0.8947\n",
    "\n",
    "Validation Loss: 0.2949\n",
    "\n",
    "Conclusion: Combining early stopping with no regularization or dropout yielded the best performance. This indicates that early stopping can effectively prevent overfitting and enhance model accuracy by halting training at the optimal point.\n",
    "\n",
    "______________________________________________________________\n",
    "______________________________________________________________\n",
    "\n",
    "\n",
    "Overall Insights\n",
    "\n",
    "Regularization Techniques: L2 regularization and dropout are effective in preventing overfitting, but the choice of dropout configuration and regularization strength significantly impacts model performance.\n",
    "\n",
    "Dropout: The absence of dropout yielded the highest accuracy, suggesting that dropout may have been too aggressive in this case.\n",
    "\n",
    "Early Stopping: Early stopping combined with no regularization or dropout achieved the best results, highlighting its importance in optimizing model performance.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
