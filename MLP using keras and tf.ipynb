{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mayank Raj 22BAI1118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -> 2 Inputs 1 Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1000, 2)\n",
      "Output shape: (1000, 1)\n",
      "Training size: 800, Testing size: 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - accuracy: 0.4315 - loss: 0.6965\n",
      "Activation Function (Hidden Layer): ReLU\n",
      "Activation Function (Output Layer): Sigmoid\n",
      "Loss: 0.6952918767929077, Accuracy: 0.46000000834465027\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate random dataset\n",
    "X = np.random.rand(1000, 2)  # Input features: 1000 samples with 2 features each\n",
    "y = np.random.randint(0, 2, size=(1000, 1))  # Binary output: 1000 samples\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print sizes of the dataset splits\n",
    "print(f'Input shape: {X.shape}')  # Shape of the input data\n",
    "print(f'Output shape: {y.shape}')  # Shape of the output data\n",
    "print(f'Training size: {X_train.shape[0]}, Testing size: {X_test.shape[0]}')  # Number of training and testing samples\n",
    "\n",
    "# Create and train model\n",
    "model = Sequential([\n",
    "    Dense(4, input_dim=2, activation='relu'),  # Hidden Layer with 4 neurons and ReLU activation\n",
    "    Dense(1, activation='sigmoid')  # Output Layer with Sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and binary cross-entropy loss\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data for 50 epochs with a batch size of 10\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print activation functions used and evaluation results\n",
    "print(f'Activation Function (Hidden Layer): ReLU')\n",
    "print(f'Activation Function (Output Layer): Sigmoid')\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "This code generates a random dataset with 1000 samples and 2 features, using binary labels for a classification problem. It splits the data into training and testing sets (80% training, 20% testing). A neural network model is then created with one hidden layer of 4 neurons using ReLU activation and an output layer using sigmoid activation for binary classification. The model is compiled with the Adam optimizer and binary cross-entropy loss, and trained for 50 epochs. Finally, the model is evaluated on the test data, reporting the loss and accuracy, which helps gauge its performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changed number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1000, 2)\n",
      "Output shape: (1000, 1)\n",
      "Training size: 800, Testing size: 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - accuracy: 0.4911 - loss: 0.6962\n",
      "Activation Function (Hidden Layer): ReLU\n",
      "Activation Function (Output Layer): Sigmoid\n",
      "Loss: 0.6953403949737549, Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Generate random dataset\n",
    "X = np.random.rand(1000, 2)  # Generate 1000 samples with 2 random input features each\n",
    "y = np.random.randint(0, 2, size=(1000, 1))  # Generate 1000 binary output labels\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print sizes of the dataset splits\n",
    "print(f'Input shape: {X.shape}')  # Total input dataset shape\n",
    "print(f'Output shape: {y.shape}')  # Total output dataset shape\n",
    "print(f'Training size: {X_train.shape[0]}, Testing size: {X_test.shape[0]}')  # Number of samples in training and testing sets\n",
    "\n",
    "# Create and train model\n",
    "model = Sequential([\n",
    "    Dense(8, input_dim=2, activation='relu'),  # Hidden layer with 8 neurons and ReLU activation\n",
    "    Dense(1, activation='sigmoid')  # Output layer with 1 neuron and sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and binary cross-entropy loss\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data for 50 epochs with a batch size of 10\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print activation functions used and evaluation results\n",
    "print(f'Activation Function (Hidden Layer): ReLU')\n",
    "print(f'Activation Function (Output Layer): Sigmoid')\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "This code generates a random dataset with 1000 samples, each having 2 features and a binary output. The data is split into 80% training and 20% testing sets. A neural network model is created with a single hidden layer consisting of 8 neurons using the ReLU activation function, and an output layer with a single neuron using the sigmoid activation function. The model is compiled using the Adam optimizer and binary cross-entropy loss function, and trained over 50 epochs with a batch size of 10. Finally, the model's performance is evaluated on the test set, yielding the loss and accuracy, which help assess the model's ability to generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1000, 2)\n",
      "Output shape: (1000, 1)\n",
      "Training size: 800, Testing size: 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - accuracy: 0.4952 - loss: 0.2521\n",
      "Activation Function (Hidden Layer): ReLU\n",
      "Activation Function (Output Layer): Sigmoid\n",
      "Loss: 0.25346922874450684, Accuracy: 0.4699999988079071\n"
     ]
    }
   ],
   "source": [
    "# Generate random dataset\n",
    "X = np.random.rand(1000, 2)  # Generate 1000 samples with 2 random input features each\n",
    "y = np.random.randint(0, 2, size=(1000, 1))  # Generate 1000 binary output labels\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print sizes of the dataset splits\n",
    "print(f'Input shape: {X.shape}')  # Total input dataset shape\n",
    "print(f'Output shape: {y.shape}')  # Total output dataset shape\n",
    "print(f'Training size: {X_train.shape[0]}, Testing size: {X_test.shape[0]}')  # Number of samples in training and testing sets\n",
    "\n",
    "# Create and train model\n",
    "model = Sequential([\n",
    "    Dense(4, input_dim=2, activation='relu'),  # Hidden layer with 4 neurons and ReLU activation\n",
    "    Dense(1, activation='sigmoid')  # Output layer with 1 neuron and sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and mean squared error loss\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data for 50 epochs with a batch size of 10\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print activation functions used and evaluation results\n",
    "print(f'Activation Function (Hidden Layer): ReLU')\n",
    "print(f'Activation Function (Output Layer): Sigmoid')\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "This code generates a random dataset of 1000 samples, each with two features, and binary labels. The data is split into 80% for training and 20% for testing. A neural network model is constructed with a single hidden layer of 4 neurons using the ReLU activation function and an output layer using the sigmoid activation function. Unlike previous versions, this model is compiled using the Adam optimizer and mean squared error (MSE) loss function, which is less common for binary classification but can be used. The model is trained for 50 epochs with a batch size of 10, and its performance is evaluated on the test set to provide the loss and accuracy, indicating the model's effectiveness in classifying the data. Using MSE as the loss function may not be optimal for this binary classification task compared to binary cross-entropy, but it still provides insights into the model's predictive capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -> 2 Inputs  2 Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1000, 2)\n",
      "Output shape: (1000, 2)\n",
      "Training size: 800, Testing size: 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.1860 - loss: 0.6448\n",
      "Activation Function (Hidden Layer): ReLU\n",
      "Activation Function (Output Layer): Softmax\n",
      "Loss: 0.6594328284263611, Accuracy: 0.22499999403953552\n"
     ]
    }
   ],
   "source": [
    "# Generate random dataset\n",
    "X = np.random.rand(1000, 2)  # Generate 1000 samples with 2 random input features each\n",
    "y = np.random.randint(0, 2, size=(1000, 2))  # Generate 1000 binary output labels for 2 classes\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print sizes of the dataset splits\n",
    "print(f'Input shape: {X.shape}')  # Total input dataset shape\n",
    "print(f'Output shape: {y.shape}')  # Total output dataset shape with 2 classes\n",
    "print(f'Training size: {X_train.shape[0]}, Testing size: {X_test.shape[0]}')  # Number of samples in training and testing sets\n",
    "\n",
    "# Create and train model\n",
    "model = Sequential([\n",
    "    Dense(4, input_dim=2, activation='relu'),  # Hidden layer with 4 neurons and ReLU activation\n",
    "    Dense(2, activation='softmax')  # Output layer with 2 neurons and softmax activation for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical cross-entropy loss\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data for 50 epochs with a batch size of 10\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print activation functions used and evaluation results\n",
    "print(f'Activation Function (Hidden Layer): ReLU')\n",
    "print(f'Activation Function (Output Layer): Softmax')\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "This code generates a random dataset with 1000 samples, each with two features and binary labels for a multi-class classification task. The labels are generated in a one-hot encoded format with two output classes. The data is split into 80% training and 20% testing. A neural network model is created with a hidden layer of 4 neurons using the ReLU activation function and an output layer with 2 neurons using the softmax activation function, suitable for multi-class classification. The model is compiled with the Adam optimizer and categorical cross-entropy loss function and trained over 50 epochs with a batch size of 10. Finally, the model's performance is evaluated on the test set, providing loss and accuracy metrics to assess its classification capabilities. This setup is appropriate for scenarios with multiple output classes, such as a multi-class classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changed loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1000, 2)\n",
      "Output shape: (1000, 2)\n",
      "Training size: 800, Testing size: 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 834us/step - accuracy: 0.3453 - loss: 0.2515\n",
      "Activation Function (Hidden Layer): ReLU\n",
      "Activation Function (Output Layer): Softmax\n",
      "Loss: 0.2511594891548157, Accuracy: 0.3449999988079071\n"
     ]
    }
   ],
   "source": [
    "# Generate random dataset\n",
    "X = np.random.rand(1000, 2)  # Generate 1000 samples with 2 random input features each\n",
    "y = np.random.randint(0, 2, size=(1000, 2))  # Generate 1000 binary output labels for 2 classes in a one-hot format\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print sizes of the dataset splits\n",
    "print(f'Input shape: {X.shape}')  # Total input dataset shape\n",
    "print(f'Output shape: {y.shape}')  # Total output dataset shape with 2 classes\n",
    "print(f'Training size: {X_train.shape[0]}, Testing size: {X_test.shape[0]}')  # Number of samples in training and testing sets\n",
    "\n",
    "# Create and train model\n",
    "model = Sequential([\n",
    "    Dense(4, input_dim=2, activation='relu'),  # Hidden layer with 4 neurons and ReLU activation\n",
    "    Dense(2, activation='softmax')  # Output layer with 2 neurons and softmax activation for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and mean squared error loss\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data for 50 epochs with a batch size of 10\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print activation functions used and evaluation results\n",
    "print(f'Activation Function (Hidden Layer): ReLU')\n",
    "print(f'Activation Function (Output Layer): Softmax')\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "This code generates a random dataset with 1000 samples, each with two features, and binary labels in a one-hot encoded format for a two-class classification problem. The data is split into 80% training and 20% testing sets. The neural network model is constructed with a hidden layer of 4 neurons using the ReLU activation function and an output layer of 2 neurons using the softmax activation function, which is suitable for multi-class classification problems. However, the model is compiled with mean squared error (MSE) as the loss function, which is typically used for regression tasks and not optimal for classification problems. The model is trained for 50 epochs with a batch size of 10, and its performance is evaluated on the test set, providing loss and accuracy metrics. While the setup demonstrates basic multi-class classification, using categorical cross-entropy as the loss function would be more appropriate for better performance in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Different Activation Function in Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1000, 2)\n",
      "Output shape: (1000, 2)\n",
      "Training size: 800, Testing size: 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.4934 - loss: 0.6785\n",
      "Activation Function (Hidden Layer): Tanh\n",
      "Activation Function (Output Layer): Softmax\n",
      "Loss: 0.6796615123748779, Accuracy: 0.47999998927116394\n"
     ]
    }
   ],
   "source": [
    "# Generate random dataset\n",
    "X = np.random.rand(1000, 2)  # Generate 1000 samples with 2 random input features each\n",
    "y = np.random.randint(0, 2, size=(1000, 2))  # Generate 1000 binary output labels for 2 classes in a one-hot format\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print sizes of the dataset splits\n",
    "print(f'Input shape: {X.shape}')  # Total input dataset shape\n",
    "print(f'Output shape: {y.shape}')  # Total output dataset shape with 2 classes\n",
    "print(f'Training size: {X_train.shape[0]}, Testing size: {X_test.shape[0]}')  # Number of samples in training and testing sets\n",
    "\n",
    "# Create and train model\n",
    "model = Sequential([\n",
    "    Dense(4, input_dim=2, activation='tanh'),  # Hidden layer with 4 neurons and Tanh activation\n",
    "    Dense(2, activation='softmax')  # Output layer with 2 neurons and softmax activation for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical cross-entropy loss\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data for 50 epochs with a batch size of 10\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print activation functions used and evaluation results\n",
    "print(f'Activation Function (Hidden Layer): Tanh')\n",
    "print(f'Activation Function (Output Layer): Softmax')\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "This code generates a random dataset with 1000 samples, each with two features and one-hot encoded binary labels for a two-class classification problem. The dataset is split into 80% for training and 20% for testing. A neural network model is created with a hidden layer of 4 neurons using the Tanh activation function, which can handle both positive and negative inputs and is useful for capturing complex relationships. The output layer uses softmax activation, suitable for multi-class classification. The model is compiled using the Adam optimizer and categorical cross-entropy loss function, which is appropriate for this classification task. The model is trained over 50 epochs with a batch size of 10, and its performance is evaluated on the test set, providing loss and accuracy metrics. This setup is well-suited for the classification problem, with appropriate choices of activation functions and loss for the given task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -> 3 Inputs 3 Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1000, 3)\n",
      "Output shape: (1000, 3)\n",
      "Training size: 800, Testing size: 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - accuracy: 0.3368 - loss: 1.1029\n",
      "Activation Function (Hidden Layer): Tanh\n",
      "Activation Function (Output Layer): Softmax\n",
      "Loss: 1.100204586982727, Accuracy: 0.33500000834465027\n"
     ]
    }
   ],
   "source": [
    "# Generate random dataset\n",
    "X = np.random.rand(1000, 3)  # Generate 1000 samples with 3 random input features each\n",
    "y = np.random.randint(0, 3, size=(1000,))  # Generate 1000 output labels with 3 possible classes\n",
    "y = to_categorical(y, num_classes=3)  # Convert labels to one-hot encoding for 3 classes\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print sizes of the dataset splits\n",
    "print(f'Input shape: {X.shape}')  # Total input dataset shape\n",
    "print(f'Output shape: {y.shape}')  # Total output dataset shape with 3 classes in one-hot format\n",
    "print(f'Training size: {X_train.shape[0]}, Testing size: {X_test.shape[0]}')  # Number of samples in training and testing sets\n",
    "\n",
    "# Create and train model\n",
    "model = Sequential([\n",
    "    Dense(8, input_dim=3, activation='tanh'),  # Hidden layer with 8 neurons and Tanh activation\n",
    "    Dense(3, activation='softmax')  # Output layer with 3 neurons and softmax activation for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical cross-entropy loss\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data for 50 epochs with a batch size of 10\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print activation functions used and evaluation results\n",
    "print(f'Activation Function (Hidden Layer): Tanh')\n",
    "print(f'Activation Function (Output Layer): Softmax')\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "This code generates a random dataset with 1000 samples, each with three features. The labels are generated for three possible classes, and then converted to a one-hot encoded format for multi-class classification. The dataset is split into 80% training and 20% testing sets. A neural network model is built with a hidden layer of 8 neurons using the Tanh activation function, which helps capture complex patterns by allowing both positive and negative values. The output layer uses softmax activation to provide probabilities across the three classes. The model is compiled using the Adam optimizer and categorical cross-entropy loss, which is well-suited for multi-class classification tasks. It is trained over 50 epochs with a batch size of 10, and its performance is evaluated on the test set to provide loss and accuracy metrics, indicating the model's effectiveness in classifying the data. This configuration is appropriate for handling classification problems with multiple classes, leveraging the Tanh and softmax activations effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Neurons in Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1000, 3)\n",
      "Output shape: (1000, 3)\n",
      "Training size: 800, Testing size: 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995us/step - accuracy: 0.2793 - loss: 1.1228\n",
      "Activation Function (Hidden Layer): ReLU\n",
      "Activation Function (Output Layer): Softmax\n",
      "Loss: 1.119808554649353, Accuracy: 0.2800000011920929\n"
     ]
    }
   ],
   "source": [
    "# Generate random dataset\n",
    "X = np.random.rand(1000, 3)  # Generate 1000 samples with 3 random input features each\n",
    "y = np.random.randint(0, 3, size=(1000,))  # Generate 1000 output labels with 3 possible classes\n",
    "y = to_categorical(y, num_classes=3)  # Convert labels to one-hot encoding for 3 classes\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print sizes of the dataset splits\n",
    "print(f'Input shape: {X.shape}')  # Total input dataset shape\n",
    "print(f'Output shape: {y.shape}')  # Total output dataset shape with 3 classes in one-hot format\n",
    "print(f'Training size: {X_train.shape[0]}, Testing size: {X_test.shape[0]}')  # Number of samples in training and testing sets\n",
    "\n",
    "# Create and train model\n",
    "model = Sequential([\n",
    "    Dense(16, input_dim=3, activation='relu'),  # Hidden layer with 16 neurons and ReLU activation\n",
    "    Dense(3, activation='softmax')  # Output layer with 3 neurons and softmax activation for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical cross-entropy loss\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data for 50 epochs with a batch size of 10\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print activation functions used and evaluation results\n",
    "print(f'Activation Function (Hidden Layer): ReLU')\n",
    "print(f'Activation Function (Output Layer): Softmax')\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "This code generates a random dataset with 1000 samples, each having three input features. The output labels are created for three possible classes, and one-hot encoding is applied for classification purposes. The dataset is split into 80% for training and 20% for testing. A neural network model is constructed with a hidden layer consisting of 16 neurons using the ReLU activation function, which is efficient for handling non-linear relationships by allowing activation only when positive. The output layer uses softmax activation to provide class probabilities across the three classes. The model is compiled with the Adam optimizer and categorical cross-entropy loss, suitable for multi-class classification tasks. It is trained for 50 epochs with a batch size of 10, and its performance is evaluated on the test set, providing loss and accuracy metrics. This setup is effective for classification tasks with multiple classes, utilizing the ReLU and softmax activations efficiently, and demonstrates how increasing the number of neurons in the hidden layer can enhance the model's capacity to learn complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Activation Function in Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1000, 3)\n",
      "Output shape: (1000, 3)\n",
      "Training size: 800, Testing size: 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3152 - loss: 1.0947 \n",
      "Activation Function (Hidden Layer): Tanh\n",
      "Activation Function (Output Layer): Softmax\n",
      "Loss: 1.0943678617477417, Accuracy: 0.3199999928474426\n"
     ]
    }
   ],
   "source": [
    "# Generate random dataset\n",
    "X = np.random.rand(1000, 3)  # Generate 1000 samples with 3 random input features each\n",
    "y = np.random.randint(0, 3, size=(1000,))  # Generate 1000 output labels with 3 possible classes\n",
    "y = to_categorical(y, num_classes=3)  # Convert labels to one-hot encoding for 3 classes\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print sizes of the dataset splits\n",
    "print(f'Input shape: {X.shape}')  # Total input dataset shape\n",
    "print(f'Output shape: {y.shape}')  # Total output dataset shape with 3 classes in one-hot format\n",
    "print(f'Training size: {X_train.shape[0]}, Testing size: {X_test.shape[0]}')  # Number of samples in training and testing sets\n",
    "\n",
    "# Create and train model\n",
    "model = Sequential([\n",
    "    Dense(8, input_dim=3, activation='tanh'),  # Hidden layer with 8 neurons and Tanh activation\n",
    "    Dense(3, activation='softmax')  # Output layer with 3 neurons and softmax activation for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical cross-entropy loss\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data for 50 epochs with a batch size of 10\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print activation functions used and evaluation results\n",
    "print(f'Activation Function (Hidden Layer): Tanh')\n",
    "print(f'Activation Function (Output Layer): Softmax')\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "This code generates a random dataset with 1000 samples, each having three input features. The output labels are generated for three possible classes, which are converted to one-hot encoding to facilitate multi-class classification. The dataset is split into 80% for training and 20% for testing. A neural network model is constructed with a hidden layer consisting of 8 neurons using the Tanh activation function, which can capture complex relationships by allowing negative, zero, and positive outputs. The output layer uses softmax activation to compute class probabilities across the three classes. The model is compiled using the Adam optimizer and categorical cross-entropy loss, which is well-suited for multi-class classification tasks. The model is trained for 50 epochs with a batch size of 10, and its performance is evaluated on the test set, providing loss and accuracy metrics. This setup is effective for handling multi-class classification problems, leveraging the Tanh and softmax activations to model complex data relationships efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (150, 4)\n",
      "Output shape: (150, 3)\n",
      "Training size: 120, Testing size: 30\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Input features from the Iris dataset\n",
    "y = iris.target  # Target labels for classification\n",
    "\n",
    "# One-hot encode the target labels\n",
    "y = to_categorical(y, num_classes=3)  # Convert labels to one-hot encoding for 3 classes\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()  # Initialize the standard scaler\n",
    "X_train = scaler.fit_transform(X_train)  # Fit and transform the training data\n",
    "X_test = scaler.transform(X_test)  # Transform the test data using the same scaler\n",
    "\n",
    "# Print sizes of the dataset splits\n",
    "print(f'Input shape: {X.shape}')  # Total input dataset shape\n",
    "print(f'Output shape: {y.shape}')  # Total output dataset shape with 3 classes in one-hot format\n",
    "print(f'Training size: {X_train.shape[0]}, Testing size: {X_test.shape[0]}')  # Number of samples in training and testing sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "This code loads the Iris dataset, which contains three classes of flowers, each described by four features. The target labels are one-hot encoded to prepare them for multi-class classification. The dataset is split into 80% training and 20% testing subsets to enable model evaluation. Standardization is applied to the input features using StandardScaler to ensure that the features have a mean of zero and a standard deviation of one, which is beneficial for training neural networks as it speeds up convergence and improves model performance. The code prints the shapes of the input and output datasets and the sizes of the training and testing sets, ensuring the data is prepared correctly for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MYK\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8667 - loss: 0.3507\n",
      "Base Model - Activation Function (Hidden Layer): ReLU\n",
      "Activation Function (Output Layer): Softmax\n",
      "Loss: 0.35067063570022583, Accuracy: 0.8666666746139526\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "model_base = Sequential([\n",
    "    Dense(8, input_dim=4, activation='relu'),  # Hidden Layer with 8 neurons and ReLU activation\n",
    "    Dense(3, activation='softmax')  # Output Layer with 3 neurons and Softmax activation for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical cross-entropy loss\n",
    "model_base.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data for 50 epochs with a batch size of 10\n",
    "model_base.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate model on the test data\n",
    "loss, accuracy = model_base.evaluate(X_test, y_test)\n",
    "\n",
    "# Print activation functions used and evaluation results\n",
    "print(f'Base Model - Activation Function (Hidden Layer): ReLU')\n",
    "print(f'Activation Function (Output Layer): Softmax')\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "This code defines and trains a neural network model for classifying Iris flowers. The model has a hidden layer with 8 neurons using the ReLU activation function, which helps the model capture complex patterns in the data by allowing for non-linearity. The output layer uses the softmax activation function to provide class probabilities for the three Iris flower classes. The model is compiled with the Adam optimizer and categorical cross-entropy loss, suitable for multi-class classification. It is trained on the Iris dataset for 50 epochs with a batch size of 10, and its performance is evaluated on the test set, reporting the loss and accuracy metrics. This setup provides a baseline for classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 2: Increased Neurons in Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9667 - loss: 0.1833\n",
      "Increased Neurons - Activation Function (Hidden Layer): ReLU\n",
      "Activation Function (Output Layer): Softmax\n",
      "Loss: 0.1833200752735138, Accuracy: 0.9666666388511658\n"
     ]
    }
   ],
   "source": [
    "# Model with increased neurons\n",
    "model_increased_neurons = Sequential([\n",
    "    Dense(16, input_dim=4, activation='relu'),  # Hidden Layer with 16 neurons and ReLU activation\n",
    "    Dense(3, activation='softmax')  # Output Layer with 3 neurons and Softmax activation for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical cross-entropy loss\n",
    "model_increased_neurons.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data for 50 epochs with a batch size of 10\n",
    "model_increased_neurons.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate model on the test data\n",
    "loss, accuracy = model_increased_neurons.evaluate(X_test, y_test)\n",
    "\n",
    "# Print activation functions used and evaluation results\n",
    "print(f'Increased Neurons - Activation Function (Hidden Layer): ReLU')\n",
    "print(f'Activation Function (Output Layer): Softmax')\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "This code defines and trains a neural network with a hidden layer containing 16 neurons, compared to the 8 neurons in the base model. Increasing the number of neurons in the hidden layer can potentially improve the model's ability to learn complex patterns in the data. The output layer remains the same with 3 neurons and softmax activation for multi-class classification. The model is compiled with the Adam optimizer and categorical cross-entropy loss and trained for 50 epochs with a batch size of 10. Its performance on the test data is evaluated, reporting loss and accuracy metrics. Comparing this model to the base model will help determine if increasing the number of neurons improves classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 3: Different Activation Function in Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9667 - loss: 0.2236\n",
      "Tanh Activation - Activation Function (Hidden Layer): Tanh\n",
      "Activation Function (Output Layer): Softmax\n",
      "Loss: 0.2236182540655136, Accuracy: 0.9666666388511658\n"
     ]
    }
   ],
   "source": [
    "# Model with Tanh activation function\n",
    "model_tanh = Sequential([\n",
    "    Dense(8, input_dim=4, activation='tanh'),  # Hidden Layer with 8 neurons and Tanh activation\n",
    "    Dense(3, activation='softmax')  # Output Layer with 3 neurons and Softmax activation for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical cross-entropy loss\n",
    "model_tanh.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data for 50 epochs with a batch size of 10\n",
    "model_tanh.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate model on the test data\n",
    "loss, accuracy = model_tanh.evaluate(X_test, y_test)\n",
    "\n",
    "# Print activation functions used and evaluation results\n",
    "print(f'Tanh Activation - Activation Function (Hidden Layer): Tanh')\n",
    "print(f'Activation Function (Output Layer): Softmax')\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "This code defines and trains a neural network using the Tanh activation function in the hidden layer, which can help in learning more complex patterns by providing output in the range of -1 to 1. The output layer uses the softmax activation function to handle multi-class classification with 3 classes. The model is compiled with the Adam optimizer and categorical cross-entropy loss, and trained for 50 epochs. After training, the model is evaluated on the test data to report its loss and accuracy, providing insight into how well the Tanh activation function performs compared to ReLU in the hidden layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
